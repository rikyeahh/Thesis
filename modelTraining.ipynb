{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# intro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run with usr/bin/python (python 3.10.12)\n",
        "BACKBONE = 'timm-resnest101e'\n",
        "N_FOLDS = 10 #number of folds\n",
        "FOLD = 3 #test fold\n",
        "PENALIZATION_LAMBDA = 0 # 0 for standard jaccard loss\n",
        "RESOLUTION = 320\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPU1Jm0-gnt-"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -y segmentation-models-pytorch\n",
        "!pip uninstall -y segmentation-models-pytorch\n",
        "!pip install --force-reinstall --no-deps segmentation-models-pytorch==0.2.1\n",
        "!pip install -U albumentations --user \n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install opencv-python\n",
        "!pip install -U numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjq5ADo7h14i"
      },
      "source": [
        "# build train/val data folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB8lTO7-e-Ul",
        "outputId": "5fe5c7f8-0466-462e-fa54-fa8ad9f1f001"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "\n",
        "if os.path.exists(\"data\"):\n",
        "    shutil.rmtree(\"data\")\n",
        "\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "    os.mkdir(\"data/torchData\")\n",
        "    os.mkdir(\"data/torchData/train\")\n",
        "    os.mkdir(\"data/torchData/trainannot\")\n",
        "    os.mkdir(\"data/torchData/val\")\n",
        "    os.mkdir(\"data/torchData/valannot\")\n",
        "    os.mkdir(\"data/torchData/test\")\n",
        "    os.mkdir(\"data/torchData/testannot\")\n",
        "\n",
        "SIZE = RESOLUTION\n",
        "n = 246\n",
        "validation_is = []\n",
        "for i in tqdm.tqdm(range(1,n)):\n",
        "    img = cv2.imread(os.path.join(\"dataset\", \"images\", f\"{i}.tif\"))\n",
        "    mask = cv2.imread(os.path.join(\"dataset\", \"masks\", f\"{i}.tif\"))\n",
        "\n",
        "    img = resize(img, (SIZE, SIZE), mode='constant', preserve_range=True).astype(np.uint8)\n",
        "    mask = resize(mask, (SIZE, SIZE), mode='constant', preserve_range=True).astype(np.uint8) > mask.mean()\n",
        "    #break\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        img = np.dstack([img, img, img])\n",
        "    if len(mask.shape) == 2:\n",
        "        mask = np.dstack([mask, mask, mask])\n",
        "    mask = ((mask) * 1).astype(np.uint8)\n",
        "\n",
        "    # if ((i % 10) == 0) or ((i % 10) == 1):\n",
        "    #     cv2.imwrite(os.path.join(\"data\", \"torchData\", \"val\", f\"{i}.png\"), img)\n",
        "    #     cv2.imwrite(os.path.join(\"data\", \"torchData\", \"valannot\", f\"{i}.png\"), mask)\n",
        "    # else:\n",
        "    #     cv2.imwrite(os.path.join(\"data\", \"torchData\", \"train\", f\"{i}.png\"), img)\n",
        "    #     cv2.imwrite(os.path.join(\"data\", \"torchData\", \"trainannot\", f\"{i}.png\"), mask)\n",
        "\n",
        "    percentage = i / n * 100\n",
        "    lower_bound = (FOLD - 1) * 10\n",
        "    upper_bound = FOLD * 10\n",
        "    if True and (percentage >= lower_bound and percentage < upper_bound):\n",
        "        validation_is.append(i)\n",
        "        cv2.imwrite(os.path.join(\"data\", \"torchData\", \"val\", f\"{i}.png\"), img)\n",
        "        cv2.imwrite(os.path.join(\"data\", \"torchData\", \"valannot\", f\"{i}.png\"), mask)\n",
        "    else:\n",
        "        cv2.imwrite(os.path.join(\"data\", \"torchData\", \"train\", f\"{i}.png\"), img)\n",
        "        cv2.imwrite(os.path.join(\"data\", \"torchData\", \"trainannot\", f\"{i}.png\"), mask)\n",
        "        \n",
        "    # mettere qui il codice per fare i fold diversamente \n",
        "\n",
        "    #cv2.imwrite(os.path.join(\"data\", \"torchData\", \"test\", f\"{i}.png\"), test_img)\n",
        "    #cv2.imwrite(os.path.join(\"data\", \"torchData\", \"testannot\", f\"{i}.png\"), test_mask)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KePRRvnzh8xE"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joExnanSgnt_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buhj48eZgnuA"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './data/torchData/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyrRRXDpgnuA"
      },
      "outputs": [],
      "source": [
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'valannot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jv2zZuagnuA"
      },
      "outputs": [],
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K8eJAiWtgnuA"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWxABD8kgnuA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQIx5yS-gnuB"
      },
      "outputs": [],
      "source": [
        "class Dataset(BaseDataset):    \n",
        "    CLASSES = ['background', 'filopodia']\n",
        "\n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = sorted(os.listdir(images_dir))\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "        \n",
        "        # convert str names to class values on masks\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        \n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        \n",
        "        #print(mask.dtype, mask.min(), mask.max())\n",
        "        # extract certain classes from mask (e.g. filopodia)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets look at data we have\n",
        "dataset = Dataset(x_train_dir, y_train_dir, classes=['filopodia'])\n",
        "\n",
        "image, mask = dataset[5] # get some sample\n",
        "print(mask.dtype, mask.min(), mask.max())\n",
        "visualize(\n",
        "    image=image, \n",
        "    filopodia_mask=mask.squeeze(),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lAoidpgKiEH-"
      },
      "source": [
        "# augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7zVOLD5gnuC"
      },
      "outputs": [],
      "source": [
        "import albumentations as albu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ncS4Qd_gnuC"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "        albu.PadIfNeeded(min_height=RESOLUTION, min_width=RESOLUTION, always_apply=True, border_mode=0),\n",
        "        # albu.RandomCrop(height=RESOLUTION, width=RESOLUTION, always_apply=True),\n",
        "        albu.GaussNoise(p=0.2),\n",
        "        albu.Perspective(p=0.5),\n",
        "        albu.OneOf([\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightnessContrast(),\n",
        "                albu.RandomGamma(p=1),\n",
        "            ],p=0.9,),\n",
        "        albu.OneOf([\n",
        "                albu.Sharpen(p=1),\n",
        "                #albu.Blur(blur_limit=3, p=1),\n",
        "                #albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],p=0.9,),\n",
        "        albu.OneOf([\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],p=0.9,),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    test_transform = [\n",
        "        albu.augmentations.geometric.resize.LongestMaxSize([RESOLUTION, RESOLUTION])\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Dataset(x_train_dir, y_train_dir, classes=['filopodia'])\n",
        "dataset = Dataset(x_valid_dir, y_valid_dir, classes=['filopodia'])\n",
        "\n",
        "image, mask = dataset[10] # get some sample\n",
        "print(mask.dtype, mask.min(), mask.max())\n",
        "visualize(\n",
        "    image=image, \n",
        "    filopodia_mask=mask.squeeze(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uDb5e7NQgnuC",
        "outputId": "6db0fe48-9e09-4fe7-f78a-00f02f5ae7f6"
      },
      "outputs": [],
      "source": [
        "# Visualize resulted augmented images and masks\n",
        "augmented_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    augmentation=get_training_augmentation(), \n",
        "    classes=['filopodia'],\n",
        ")\n",
        "\n",
        "# same image with different random transforms\n",
        "for i in range(3):\n",
        "    image, mask = augmented_dataset[0]\n",
        "    print(image.shape)\n",
        "    visualize(image=image, mask=mask.squeeze(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# create counting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counting_model = torch.load(\"best_counting_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# create segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IxTi7S-gnuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXY8iArXgnuD"
      },
      "outputs": [],
      "source": [
        "ENCODER = BACKBONE\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = ['filopodia']\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_2DWZ4YgnuD",
        "outputId": "3b441cfd-5c51-4643-e89e-122cfe0bdb4c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    augmentation=get_training_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    augmentation=None, \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import skimage\n",
        "from skimage.morphology import thin\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "\n",
        "def find_branchpoints(skeleton):\n",
        "    #skeleton = skeleton.astype(int)\n",
        "    return find_endpoints(skeleton) - 2\n",
        "\n",
        "def find_endpoints(img):\n",
        "    # Find row and column locations that are non-zero\n",
        "    (rows,cols) = np.nonzero(img)\n",
        "\n",
        "    # Initialize empty list of co-ordinates\n",
        "    skel_coords = []\n",
        "\n",
        "    # For each non-zero pixel...\n",
        "    for (r,c) in zip(rows,cols):\n",
        "\n",
        "        # Extract an 8-connected neighbourhood\n",
        "        (col_neigh,row_neigh) = np.meshgrid(np.array([c-1,c,c+1]), np.array([r-1,r,r+1]))\n",
        "\n",
        "        # Cast to int to index into image\n",
        "        col_neigh = col_neigh.astype('int')\n",
        "        row_neigh = row_neigh.astype('int')\n",
        "\n",
        "        # Convert into a single 1D array and check for non-zero locations\n",
        "        pix_neighbourhood = img[row_neigh,col_neigh].ravel() != 0\n",
        "\n",
        "        # If the number of non-zero locations equals 2, add this to our list of co-ordinates\n",
        "        if np.sum(pix_neighbourhood) == 2:\n",
        "            skel_coords.append((r,c))\n",
        "\n",
        "    return len(skel_coords)\n",
        "\n",
        "def detect_fused(img):\n",
        "\n",
        "    n_fused = 0\n",
        "    n_single = 0\n",
        "\n",
        "    img_thinned = thin(img) # or skeletonize, small difference\n",
        "    img_thinned[0,:] = 0\n",
        "    img_thin_labeled = skimage.measure.label(img_thinned.astype(np.uint8), connectivity=2)\n",
        "    img_labeled = skimage.measure.label(img.astype(np.uint8), connectivity=2)\n",
        "    stats_bbox = skimage.measure.regionprops(img_thin_labeled.astype(np.uint8))\n",
        "    # results to fill\n",
        "    fused_image = np.zeros_like(img)\n",
        "    singles_image = np.zeros_like(img)\n",
        "    finish = np.zeros_like(img)\n",
        "\n",
        "    for i in range(0, len(stats_bbox)):\n",
        "\n",
        "        bbox = stats_bbox[i].bbox\n",
        "        # take thinned branch region\n",
        "        bbox_region = img_thin_labeled[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
        "\n",
        "        # take its largest connected component in case multiple accidentally are in that bounding box\n",
        "        value_counts = Counter(bbox_region.flatten()).most_common()\n",
        "        most_frequent_value = value_counts[1][0] if len(value_counts) > 1 else value_counts[0][0]\n",
        "        bbox_region = (bbox_region == most_frequent_value) * 1\n",
        "\n",
        "        # if into that bounding box #branchpoints > 1 AND #endpoints >= 4, it is a FUSED filopodia\n",
        "        bbox_region_padded = np.pad(bbox_region, pad_width=4, mode='constant', constant_values=0)\n",
        "        n_endpoints = find_endpoints(bbox_region_padded)\n",
        "        n_branchpoints = find_branchpoints(bbox_region_padded)\n",
        "        is_fused = n_branchpoints > 1 and n_endpoints >= 4\n",
        "\n",
        "        # mark FUSED and SINGLE regions with 2 different values\n",
        "        if is_fused:\n",
        "            fused_image += (img_labeled == (i + 1))\n",
        "            n_fused += 1\n",
        "        else:\n",
        "            singles_image += (img_labeled == (i + 1))\n",
        "            n_single += 1\n",
        "\n",
        "        finish = singles_image + fused_image * 2\n",
        "\n",
        "    return finish, n_single, n_fused\n",
        "\n",
        "def num_filopodia_demerged(mask):\n",
        "    thinned = thin(mask)\n",
        "    img_thin_labeled = skimage.measure.label(thinned.astype(np.uint8), connectivity=2)\n",
        "    stats_bbox = skimage.measure.regionprops(img_thin_labeled.astype(np.uint8))\n",
        "    filopodia_count = 0\n",
        "    for i in range(0, len(stats_bbox)):\n",
        "        bbox = stats_bbox[i].bbox\n",
        "        bbox_region = img_thin_labeled[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
        "        value_counts = Counter(bbox_region.flatten()).most_common()\n",
        "        most_frequent_value = value_counts[1][0] if len(value_counts) > 1 else value_counts[0][0]\n",
        "        bbox_region = (bbox_region == most_frequent_value) * 1\n",
        "\n",
        "        # if into that bounding box #branchpoints > 1 AND #endpoints >= 4, it is a FUSED filopodia\n",
        "        bbox_region_padded = np.pad(bbox_region, pad_width=4, mode='constant', constant_values=0)\n",
        "        n_endpoints = find_endpoints(bbox_region_padded)\n",
        "        \n",
        "        filopodia_count += (n_endpoints - 1)\n",
        "    return filopodia_count\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # y_true and y_pred are batches, calculate single losses\n",
        "    filopodia_penalization = 0\n",
        "    #print(\"loss RESOLUTIONZE: \", len(y_pred))\n",
        "    for i in range(0, len(y_pred)):\n",
        "        pred = y_pred[i]\n",
        "        true = y_true[i]\n",
        "        pred = (pred.cpu().detach().numpy()[0].reshape((RESOLUTION, RESOLUTION)) > 0.5).astype(np.float64)\n",
        "        true = true.cpu().detach().numpy()[0].reshape((RESOLUTION, RESOLUTION))\n",
        "        n_filo_pred = num_filopodia_demerged(pred) # o counting_model(torch.Tensor(pred))\n",
        "        n_filo_true = num_filopodia_demerged(true)\n",
        "        if n_filo_true > 0 and n_filo_pred > 0:\n",
        "            filopodia_penalization += np.abs(np.log(n_filo_pred / n_filo_true))\n",
        "        else:\n",
        "            filopodia_penalization += 0 \n",
        "\n",
        "    return filopodia_penalization\n",
        "\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "\n",
        "    iou_loss = smp.utils.losses.JaccardLoss(eps=0.1)\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.__name__ = \"jaccard_loss\"\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        iou = self.iou_loss.forward(output, target)\n",
        "        penalization = custom_loss(target, output) * PENALIZATION_LAMBDA\n",
        "        penalization = torch.tensor(penalization).to(DEVICE)\n",
        "        return iou + penalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = smp.utils.losses.JaccardLoss(eps=0.1) if PENALIZATION_LAMBDA == 0 else CustomLoss()\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oY-_1OXFiQm0"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create epoch runners \n",
        "# it is a simple loop of iterating over dataloader's samples\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CgXTojaiwgk",
        "outputId": "74ccb57e-aa7e-4a52-ca3e-dd24dbfa728d"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7hp1sppgnuD",
        "outputId": "678b9548-20ab-4e6b-80c3-599f865caafa"
      },
      "outputs": [],
      "source": [
        "max_score = 0\n",
        "train_loss_progession = []\n",
        "val_loss_progression = []\n",
        "\n",
        "for i in range(0, 100):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "    train_loss_progession.append(train_logs['jaccard_loss'])\n",
        "    val_loss_progression.append(valid_logs['jaccard_loss'])\n",
        "\n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')\n",
        "\n",
        "    # if i % 5 == 0:\n",
        "    #     optimizer.param_groups[0]['lr'] = 0.01\n",
        "    # else:\n",
        "    #     optimizer.param_groups[0]['lr'] = 0.0001\n",
        "        \n",
        "    if i > 70:\n",
        "        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.8\n",
        "        print('Decrease decoder learning rate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWUsJ0En47_J"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss_progession)\n",
        "plt.plot(val_loss_progression)\n",
        "plt.legend(labels=[\"train loss\", \"val loss\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "631JUVEggnuD"
      },
      "source": [
        "# Test best saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHJJRns8gnuE"
      },
      "outputs": [],
      "source": [
        "# load best saved checkpoint\n",
        "best_model = torch.load('./best_model.pth')\n",
        "\n",
        "ENCODER = 'timm-resnest101e'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = ['filopodia']\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tTf5dLAgnuE"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './data/torchData/'\n",
        "x_test_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "# create test dataset\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    augmentation=None, \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=[\"filopodia\"],\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDrmjphIgnuE"
      },
      "outputs": [],
      "source": [
        "# evaluate model on test set\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model=best_model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "logs = test_epoch.run(test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gQHgKfOYgnuE"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = './data/torchData/'\n",
        "x_test_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "\n",
        "# create test dataset\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    augmentation=None, \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=[\"filopodia\"],\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQCTIIB6gnuE"
      },
      "outputs": [],
      "source": [
        "for i in range(len(test_dataset)):\n",
        "    n = i\n",
        "    \n",
        "    image, gt_mask = test_dataset[n]\n",
        "    \n",
        "    gt_mask = gt_mask.squeeze().astype(bool)\n",
        "    \n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    print(image.shape, image.dtype, image.min(), image.max(), image.mean())\n",
        "    pr_mask = best_model.predict(x_tensor)\n",
        "    pr_mask = (pr_mask.squeeze().cpu().numpy() > 0.999)\n",
        "    # todo small erosion\n",
        "    pr_mask = cv2.erode(pr_mask.astype(np.uint8), cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))) * 255\n",
        "        \n",
        "    visualize(image=image[0,:,:], ground_truth_mask=gt_mask, predicted_mask=pr_mask, diff=(gt_mask ^ pr_mask))\n",
        "    print(str(i + (FOLD - 1) * len(test_dataset)))\n",
        "    cv2.imwrite(\"./finalPredictions512/\" + str(i + (FOLD - 1) * len(test_dataset)) + \".png\", pr_mask)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZ9sD-CYiPQ"
      },
      "source": [
        "# measure metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-NEfVvfcuX8"
      },
      "outputs": [],
      "source": [
        "def iou(prediction, true_mask):\n",
        "    intersection = np.logical_and(prediction, true_mask).sum()\n",
        "    union = np.logical_or(prediction, true_mask).sum()\n",
        "    iou_score = intersection / union\n",
        "    return iou_score\n",
        "\n",
        "def dice(prediction, true_mask):\n",
        "    intersection = np.logical_and(prediction, true_mask).sum()\n",
        "    dice_score = (2. * intersection) / (prediction.sum() + true_mask.sum())\n",
        "    return dice_score\n",
        "\n",
        "def precision(prediction, true_mask):\n",
        "    true_positives = np.logical_and(prediction, true_mask).sum()\n",
        "    false_positives = np.logical_and(prediction, np.logical_not(true_mask)).sum()\n",
        "    precision_score = true_positives / (true_positives + false_positives)\n",
        "    return precision_score\n",
        "\n",
        "\n",
        "def recall(prediction, true_mask):\n",
        "    true_positives = np.logical_and(prediction, true_mask).sum()\n",
        "    false_negatives = np.logical_and(np.logical_not(prediction), true_mask).sum()\n",
        "    if int(true_positives + false_negatives) == 0:\n",
        "        return 0\n",
        "    recall_score = true_positives / (true_positives + false_negatives)\n",
        "    return recall_score\n",
        "\n",
        "def f1_score(prediction, true_mask):\n",
        "    p = precision(prediction, true_mask)\n",
        "    r = recall(prediction, true_mask)\n",
        "    if precision == 0:\n",
        "        return 0\n",
        "    f1 = 2 * (p * r) / (p + r)\n",
        "    return f1\n",
        "\n",
        "def mse(prediction, true_mask):\n",
        "    mse_score = np.mean((prediction - true_mask) ** 2)\n",
        "    return mse_score\n",
        "\n",
        "def num_filopodia_blobs(mask):\n",
        "    return skimage.measure.label(mask)\n",
        "\n",
        "def num_filopodia_demerged(mask):\n",
        "    thinned = thin(mask)\n",
        "    img_thin_labeled = skimage.measure.label(thinned.astype(np.uint8), connectivity=2)\n",
        "    stats_bbox = skimage.measure.regionprops(img_thin_labeled.astype(np.uint8))\n",
        "    filopodia_count = 0\n",
        "    for i in range(0, len(stats_bbox)):\n",
        "        bbox = stats_bbox[i].bbox\n",
        "        bbox_region = img_thin_labeled[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
        "\n",
        "        value_counts = Counter(bbox_region.flatten()).most_common()\n",
        "        most_frequent_value = value_counts[1][0] if len(value_counts) > 1 else value_counts[0][0]\n",
        "        bbox_region = (bbox_region == most_frequent_value) * 1\n",
        "\n",
        "        # if into that bounding box #branchpoints > 1 AND #endpoints >= 4, it is a FUSED filopodia\n",
        "        bbox_region_padded = np.pad(bbox_region, pad_width=4, mode='constant', constant_values=0)\n",
        "        n_endpoints = find_endpoints(bbox_region_padded)\n",
        "        \n",
        "        filopodia_count += n_endpoints - 1\n",
        "    return filopodia_count\n",
        "\n",
        "def filopodia_length_sum(mask):\n",
        "    return np.count_nonzero(thin(mask))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA3HkW1_YlG4"
      },
      "outputs": [],
      "source": [
        "IOUs, DICEs, PRECISIONs, RECALLs, F1SCOREs, MSEs = [],[],[],[],[],[]\n",
        "filo_N_diffs, filo_N_abs_diffs, filo_len_diffs, filo_len_abs_diffs = [],[],[],[]\n",
        "single_filo_N_diff, single_filo_N_abs_diff, merged_filo_N_diff, merged_filo_N_abs_diff = [],[],[],[]\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    n = i\n",
        "    \n",
        "    image, gt_mask = test_dataset[n]\n",
        "    plt.imshow(image[0,:,:]), plt.show()\n",
        "    \n",
        "    gt_mask = gt_mask.squeeze()\n",
        "    \n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    pr_mask = best_model.predict(x_tensor)\n",
        "    pr_mask = (pr_mask.squeeze().cpu().numpy() > 0.99)\n",
        "\n",
        "    pred = pr_mask\n",
        "    mask = gt_mask\n",
        "\n",
        "    contours, _ = cv2.findContours(pred.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    filtered_contours = []\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area > 25:\n",
        "            filtered_contours.append(contour)\n",
        "    filtered_image = np.zeros_like(pred, dtype=float)\n",
        "    cv2.drawContours(filtered_image, filtered_contours, -1, 255, thickness=cv2.FILLED)\n",
        "    #pred = filtered_image\n",
        "    \n",
        "    fused_pred, n_single_p, n_fused_p = detect_fused(pred)\n",
        "    fused_true, n_single_t, n_fused_t = detect_fused(mask)\n",
        "\n",
        "    IOUs.append(iou(pred, mask))\n",
        "    DICEs.append(dice(pred, mask))\n",
        "    PRECISIONs.append(precision(pred, mask))\n",
        "    RECALLs.append(recall(pred, mask))\n",
        "    F1SCOREs.append(f1_score(pred, mask))\n",
        "    MSEs.append(mse(pred, mask))\n",
        "    filo_N_diffs.append(num_filopodia_demerged(pred) - num_filopodia_demerged(mask))\n",
        "    filo_N_abs_diffs.append(abs(num_filopodia_demerged(pred) - num_filopodia_demerged(mask)))\n",
        "    filo_len_diffs.append(filopodia_length_sum(pred) - filopodia_length_sum(mask))\n",
        "    filo_len_abs_diffs.append(abs(filopodia_length_sum(pred) - filopodia_length_sum(mask)))\n",
        "    single_filo_N_diff.append(n_single_p - n_single_t)\n",
        "    single_filo_N_abs_diff.append(abs(n_single_p - n_single_t))\n",
        "    merged_filo_N_diff.append(n_fused_p - n_fused_t)\n",
        "    merged_filo_N_abs_diff.append(abs(n_fused_p - n_fused_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, check if all lists have the same length\n",
        "lengths = [len(lst) for lst in [IOUs, DICEs, PRECISIONs, RECALLs, F1SCOREs, MSEs,\n",
        "                                filo_N_diffs, filo_N_abs_diffs, filo_len_diffs, filo_len_abs_diffs,\n",
        "                                single_filo_N_diff, single_filo_N_abs_diff,\n",
        "                                merged_filo_N_diff, merged_filo_N_abs_diff]]\n",
        "assert all(length == lengths[0] for length in lengths), \"All lists must have the same length\"\n",
        "\n",
        "lists = [IOUs, DICEs, PRECISIONs, RECALLs, F1SCOREs, MSEs,\n",
        "         filo_N_diffs, filo_N_abs_diffs, filo_len_diffs, filo_len_abs_diffs,\n",
        "         single_filo_N_diff, single_filo_N_abs_diff,\n",
        "         merged_filo_N_diff, merged_filo_N_abs_diff]\n",
        "\n",
        "for i in range(len(IOUs)):\n",
        "    values = [lst[i] for lst in lists]\n",
        "    print(\", \".join(map(str, values)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FiwW7oLdzf6"
      },
      "outputs": [],
      "source": [
        "print(\"IOU\", np.mean(IOUs), \"±\", np.std(IOUs))\n",
        "print(\"DICE\", np.mean(DICEs), \"±\", np.std(DICEs))\n",
        "print(\"PRECISION\", np.nanmean(PRECISIONs), \"±\", np.nanstd(PRECISIONs))\n",
        "print(\"RECALL\", np.mean(RECALLs), \"±\", np.std(RECALLs))\n",
        "print(\"F1\", np.nanmean(F1SCOREs), \"±\", np.nanstd(F1SCOREs))\n",
        "print(\"MSE\", np.mean(MSEs), \"±\", np.std(MSEs))\n",
        "print(\"Filo # difference\", np.mean(filo_N_diffs), \"±\", np.std(filo_N_diffs))\n",
        "print(\"Filo # abs difference\", np.mean(filo_N_abs_diffs), \"±\", np.std(filo_N_abs_diffs))\n",
        "print(\"Filo len difference\", np.mean(filo_len_diffs), \"±\", np.std(filo_len_diffs))\n",
        "print(\"Filo len abs difference\", np.mean(filo_len_abs_diffs), \"±\", np.std(filo_len_abs_diffs))\n",
        "print(\"Single filo # diff\", np.mean(single_filo_N_diff), \"±\", np.std(single_filo_N_diff))\n",
        "print(\"Single filo # abs diff\", np.mean(single_filo_N_abs_diff), \"±\", np.std(single_filo_N_abs_diff))\n",
        "print(\"Fused filo # diff\", np.mean(merged_filo_N_diff), \"±\", np.std(merged_filo_N_diff))\n",
        "print(\"Fused filo # abs diff\", np.mean(merged_filo_N_abs_diff), \"±\", np.std(merged_filo_N_abs_diff))\n",
        "print(np.mean(IOUs), \"±\", np.std(IOUs), \",\",\n",
        "        np.mean(DICEs), \"±\", np.std(DICEs), \",\",\n",
        "        np.nanmean(PRECISIONs), \"±\", np.nanstd(PRECISIONs), \",\",\n",
        "        np.mean(RECALLs), \"±\", np.std(RECALLs), \",\",\n",
        "        np.nanmean(F1SCOREs), \"±\", np.nanstd(F1SCOREs), \",\",\n",
        "        np.mean(MSEs), \"±\", np.std(MSEs), \",\",\n",
        "        np.mean(filo_N_diffs), \"±\", np.std(filo_N_diffs), \",\",\n",
        "        np.mean(filo_N_abs_diffs), \"±\", np.std(filo_N_abs_diffs), \",\",\n",
        "        np.mean(filo_len_diffs), \"±\", np.std(filo_len_diffs), \",\",\n",
        "        np.mean(filo_len_abs_diffs), \"±\", np.std(filo_len_abs_diffs), \",\",\n",
        "        np.mean(single_filo_N_diff), \"±\", np.std(single_filo_N_diff) , \",\",\n",
        "        np.mean(single_filo_N_abs_diff), \"±\", np.std(single_filo_N_abs_diff) , \",\",\n",
        "        np.mean(merged_filo_N_diff), \"±\", np.std(merged_filo_N_diff) , \",\",\n",
        "        np.mean(merged_filo_N_abs_diff), \"±\", np.std(merged_filo_N_abs_diff) , \",\",)\n",
        "means = [np.mean(IOUs), np.mean(DICEs), np.nanmean(PRECISIONs), np.mean(RECALLs), np.nanmean(F1SCOREs), np.mean(MSEs), np.mean(filo_N_diffs), np.mean(filo_N_abs_diffs), np.mean(filo_len_diffs), np.mean(filo_len_abs_diffs), np.mean(single_filo_N_diff), np.mean(single_filo_N_abs_diff), np.mean(merged_filo_N_diff), np.mean(merged_filo_N_abs_diff)]\n",
        "print(', '.join(['{:.3f}'.format(mean) for mean in means]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lAoidpgKiEH-",
        "631JUVEggnuD"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
